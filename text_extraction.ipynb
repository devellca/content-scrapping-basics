{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sqlalchemy import create_engine\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import urllib\n",
    "\n",
    "# import ssdeep\n",
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 93\n"
     ]
    }
   ],
   "source": [
    "#get url's content from solr\n",
    "\n",
    "def get_content(url):\n",
    "    df = pd.DataFrame()\n",
    "    response = requests.get(url,timeout=5)\n",
    "    data = json.loads(response.text)\n",
    "    cursor = '*'\n",
    "    i=0\n",
    "    while (len(data['response']['docs'])==100):\n",
    "        i+=1\n",
    "        response = requests.get(url,timeout=5)\n",
    "        data = json.loads(response.text)\n",
    "        cursor = data['nextCursorMark']\n",
    "        try:\n",
    "            temp = pd.DataFrame.from_dict(data['response']['docs'])\n",
    "            df = pd.concat([df,temp],ignore_index=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(i,len(temp))\n",
    "    if len(df)>0:\n",
    "        return df\n",
    "df=get_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating tree structure for all url's based on the child and parent url's\\\n",
    "#   to find the leaf nodes\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, val, count):\n",
    "        self.val = val\n",
    "        self.count = count\n",
    "        self.children = dict()\n",
    "        \n",
    "        \n",
    "class Tree:\n",
    "    import re\n",
    "    def __init__(self, l):\n",
    "        self.root = Node(\"\", 0)\n",
    "        self._build(l)\n",
    "\n",
    "    def _build(self, l):\n",
    "\n",
    "        for row in l:\n",
    "#             print(row)\n",
    "#             path_string = re.search(r\"en_us/(.*).html(.*)\", row)\n",
    "            path_string=re.search(r\"(.*).html(.*)\",row)\n",
    "#             print(path_string)\n",
    "            if path_string:\n",
    "                path = path_string.group(1).split('/')\n",
    "#                 print(path)\n",
    "                current_node = self.root\n",
    "                for node in path:\n",
    "                    if node in current_node.children:\n",
    "                        current_node = current_node.children[node]\n",
    "                    else:\n",
    "                        current_node.children[node] = Node(node, 1)\n",
    "                        current_node = current_node.children[node]\n",
    "                        \n",
    "\n",
    "    def getLeaf(self, showCount=False):\n",
    "        \"\"\"\n",
    "        return all leaf sites\n",
    "\n",
    "        showCount: Boolean, if set to True, the list returned will contain the number of times leaf sites are visited\n",
    "        \"\"\"\n",
    "\n",
    "        def _helper(leaf, path, node, showCount=False):\n",
    "#             print(node.children)\n",
    "            if not node:\n",
    "                return\n",
    "            if len(node.children) == 0:\n",
    "                if showCount:\n",
    "                    leaf.append([path+'/'+node.val, node.count])\n",
    "                    \n",
    "                else:\n",
    "#                     print(node.val)\n",
    "                    leaf.append([path+'/'+node.val])\n",
    "                return\n",
    "            else:\n",
    "                for child in node.children:\n",
    "                    _helper(leaf, path+'/'+node.val, node.children[child], showCount)\n",
    "\n",
    "        leaf = []\n",
    "#         transition={}\n",
    "        _helper(leaf, \"\", self.root, showCount)\n",
    "\n",
    "        return (leaf)\n",
    "    \n",
    "# path_all=['https://one.walmart.com'+i for i in list(df['url'].head(10))]\n",
    "test=Tree(list(df['url']))\n",
    "\n",
    "leaf_nodes=test.getLeaf()\n",
    "\n",
    "leaves=['/'+i[0][1:]+'.html' for i in leaf_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_parser(url):\n",
    "#     print(type(url))\n",
    "        username = 'username'\n",
    "        password = 'password'\n",
    "\n",
    "        response=requests.get(url, auth=HTTPBasicAuth(username, password))\n",
    "        if response.status_code != 200:\n",
    "            response=requests.get(url, auth=HTTPBasicAuth(username, password))\n",
    "            text=response.text.encode('utf-8')\n",
    "            soup = BeautifulSoup(text,'html.parser')\n",
    "            \n",
    "        try:    \n",
    "            response1=requests.get(url.split('.html')[0]+'.children.json', auth=HTTPBasicAuth(username, password))\n",
    "            response_list=json.loads(response1.text.encode('utf-8'))\n",
    "            for i in response_list:\n",
    "                if(i.get('cq:lastModified')!=None):\n",
    "                    last_modified=i['cq:lastModified']\n",
    "                    return last_modified,soup\n",
    "            return soup\n",
    "        except Exception as e:\n",
    "            return soup\n",
    "        else:\n",
    "            return 'not a valid format'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#parse html to get text data\n",
    "\n",
    "def subtitle1(soup):\n",
    "    data={'text':'','subtitles':''}\n",
    "    classes = []\n",
    "    subtitle_classes=[]\n",
    "    text_classes=[]\n",
    "    header_classes=[]\n",
    "    footer_classes=[]\n",
    "    hidden_classes=[]\n",
    "    sub=[]\n",
    "    for element in soup.find_all(class_=True):\n",
    "        classes.extend(element[\"class\"])\n",
    "    classes=remove_duplicates(classes)\n",
    "    for item in classes:\n",
    "        if item.find(\"title\")!=-1:\n",
    "            if \"header\" not in item:\n",
    "#                 print(item)\n",
    "                subtitle_classes.append(item)\n",
    "    for item in classes:\n",
    "        if item.find(\"text\")!=-1:\n",
    "                text_classes.append(item)\n",
    "    for item in classes:\n",
    "        if item.find(\"header\")!=-1:\n",
    "                header_classes.append(item)\n",
    "    for item in classes:\n",
    "        if item.find(\"footer\")!=-1:\n",
    "                text_classes.append(item)\n",
    "    for item in classes:\n",
    "        if item.find(\"hidden\")!=-1:\n",
    "                hidden_classes.append(item)\n",
    "    \n",
    "#killing unneccessary headers, footer, style components etc\n",
    "    for script in soup([\"title\",\"script\", \"style\",\"label\",'h',\"head\"]): #delete the title as well\n",
    "    #     print(script)\n",
    "        script.decompose()    # rip it out\n",
    "    for item in header_classes:\n",
    "        for i in soup.findAll(class_=item):\n",
    "            i.decompose()\n",
    "    for item in footer_classes:\n",
    "        for i in soup.findAll(class_=item):\n",
    "            i.decompose()\n",
    "    for item in hidden_classes:\n",
    "        for i in soup.findAll(class_=item):\n",
    "            i.decompose()\n",
    "    for item in classes:\n",
    "        if item.find(\"not-to-translate\")!=-1:\n",
    "            for i in soup.findAll(class_=item):\n",
    "                i.decompose()\n",
    "        \n",
    "    \n",
    "#     complete_text = soup.get_text()\n",
    "# #     print(text)\n",
    "#     # break into lines and remove leading and trailing space on each\n",
    "#     lines = (line.strip() for line in complete_text.splitlines())\n",
    "#     # # break multi-headlines into a line each\n",
    "#     chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "#     # # drop blank lines\n",
    "#     complete_text = ' '.join(chunk.replace('\\n','').replace(u'\\xa0',' ').replace('\\t',' ') for chunk in chunks if chunk)\n",
    "\n",
    "#find subtitles in the pages\n",
    "    for ite in subtitle_classes:\n",
    "        for i in soup.find_all(class_=ite):\n",
    "            if(\"notifications\" not in ite):\n",
    "                a=i.get_text().replace('\\n',' ').replace('\\t',' ')\n",
    "                a=re.sub(' +',' ',a).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "#                 re.sub(r\"[^a-zA-Z0-9\\s./]+\",\" \",\n",
    "                if a!=' ':\n",
    "                    sub.append(a)\n",
    "                i.decompose()\n",
    "        for i in soup.find_all(re.compile(r'h[123]')):\n",
    "            a=i.get_text().replace('\\n',' ').replace('\\t',' ')\n",
    "            a=re.sub(' +',' ',a).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "            if a!=' ':\n",
    "                sub.append(a)\n",
    "            i.decompose()\n",
    "#         for i in soup.find_all(class_=\"white-text\"):\n",
    "#             print(i.get_text())\n",
    "#             a=i.get_text().replace('\\n',' ').replace(u'\\xa0',' ').replace('\\t',' ')\n",
    "#             sub.append(a)\n",
    "#             i.decompose()\n",
    "    \n",
    "#     sub=remove_duplicates(sub)\n",
    "    sub=','.join(list(filter(None, sub)))\n",
    "\n",
    "    \n",
    "#extract text content of the pages\n",
    "    t=[]\n",
    "    for ite in text_classes:\n",
    "        for i in soup.findAll(class_=ite):\n",
    "            if(\"notifications\" not in ite):\n",
    "                a=i.get_text().replace('\\n',' ').replace('\\t',' ')\n",
    "                if a!=' ':\n",
    "                    t.append(a)\n",
    "                i.decompose()\n",
    "        for i in soup.find_all(re.compile(r'h[123]')):\n",
    "            a=i.get_text().replace('\\n',' ').replace('\\t',' ')\n",
    "            if a!=' ':\n",
    "                t.append(a)\n",
    "            i.decompose()\n",
    "\n",
    "    for i in soup(\"p\"):\n",
    "#         print('hello')\n",
    "#         print(\"p::\",i.get_text())\n",
    "        a=i.get_text().replace('\\n',' ').replace('\\t',' ')\n",
    "#         a=a.replace('   ','')\n",
    "        t.append(a)\n",
    "        i.decompose()\n",
    "   \n",
    "    for i in soup(\"div\", class_=\"tile parbase section\"):\n",
    "#         print(\"tile parbase:::\",i.get_text())\n",
    "        a=i.get_text().replace('\\n',' ').replace('\\t',' ')\n",
    "#         a=a.replace('   ','')\n",
    "        t.append(a)\n",
    "        i.decompose()\n",
    "  \n",
    "# print(6)\n",
    "        \n",
    "\n",
    "    for i in soup(\"div\",class_='caption image-width caption-alignment center imagefont-regular caption-above'):\n",
    "#         print(\"caption image:::\",i.get_text())\n",
    "        a=i.get_text().replace('\\n',' ').replace(u'\\xa0',' ').replace('\\t',' ')\n",
    "#         a=a.replace('   ','')\n",
    "        t.append(a)\n",
    "#         print(7)\n",
    "        i.decompose()\n",
    "        \n",
    "#     print(\"l\",t)\n",
    "#     print(t)\n",
    "    t = ' '.join(list(filter(None, t)))\n",
    "#     print(t)\n",
    "    \n",
    "        \n",
    "    for i in soup.find_all(re.compile(r'h\\d+')):\n",
    "        #         print(i)\n",
    "\n",
    "        try:\n",
    "            if i.find_parent().attrs['class'] != ['dv-item-info']:\n",
    "                #                 print('i decomposed:{}'.format(i))\n",
    "                i.decompose()\n",
    "        except:\n",
    "            pass\n",
    "#     print(soup.get_text())   \n",
    "\n",
    "# # kill all script and style elements\n",
    "    \n",
    "    \n",
    "    #kill header    \n",
    "    for script in soup(\"div\", class_=\"hide-on-print\"):\n",
    "        script.decompose()\n",
    "\n",
    "    #kill header    \n",
    "    for script in soup(\"div\", class_=\"article-tmpl-full-width section\"):\n",
    "        script.decompose()\n",
    "\n",
    "    #kill the directory tab\n",
    "    for script in soup(\"div\", class_=\"breadcrumb section\"):\n",
    "        script.decompose()\n",
    "\n",
    "    #kill the navegation tab \n",
    "    for script in soup(\"div\", class_=\"in-page-navigation ddl align-center\"):\n",
    "        script.decompose()       \n",
    "\n",
    "    for script in soup(\"div\",id=\"knowledgecheck\",class_='in-page-navigation-sections'):\n",
    "        script.decompose()\n",
    "\n",
    "\n",
    "    for script in soup(\"div\", class_=\"navigation-section-title\"):\n",
    "        script.decompose()    \n",
    "\n",
    "   \n",
    "    for script in soup(\"div\",class_=\"output_wrapper\"):\n",
    "        script.decompose()\n",
    "\n",
    "    for script in soup(\"div\",class_='html-component section'):\n",
    "        script.decompose()\n",
    "\n",
    "    for script in soup(\"div\",id=\"acs-commons-env-indicator\"):\n",
    "        script.decompose()\n",
    "\n",
    "    for script in soup(\"div\",class_=\"margin-bottom-0 table-component adp-richtext-circle\"):\n",
    "        script.decompose()\n",
    "\n",
    "\n",
    "        \n",
    "     #get text\n",
    "  \n",
    "    text = soup.get_text()\n",
    "#     print(text)\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # # drop blank lines\n",
    "    text = ' '.join(chunk.replace('\\n','').replace('\\t',' ') for chunk in chunks if chunk)\n",
    "    data['text'] = (re.sub(' +', ' ',t + \" \" + text)).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "# re.sub(r\"[^a-zA-Z0-9,\\s./]+\", \" \",\n",
    "#     data['text']=(re.sub(' +', ' ',re.sub(r\"[^a-zA-Z0-9\\s./']+\",\" \",t+\" \"+text)))\n",
    "    data['subtitles']=(sub)\n",
    "#     data['complete_text']=(re.sub(' +', ' ',re.sub(r\"[^a-zA-Z0-9\\s./]+\",\" \",complete_text)))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtitle1(soup)\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "'''remove punctuation, lowercase, stem'''\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
    "\n",
    "def cosine_sim(text1, text2):\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#workflow for parsing a web url and getting text content and subtitles out of it\n",
    "broken=[]\n",
    "data=[]\n",
    "data_updated=[]\n",
    "failed_urls=[]\n",
    "count=0\n",
    "today = datetime.today()\n",
    "\n",
    "\n",
    "def myfn(url):\n",
    "#     conn = connector.get_connector(name = 'AAL_cognitive_service_dev')\n",
    "    try:\n",
    "        i=url[1]\n",
    "#         modified_date=url['last_modified'].to_numpy()[0]\n",
    "        modified_date=url[0]\n",
    "        try:\n",
    "            date_modified,soup=web_parser(i)\n",
    "        except:\n",
    "            soup=web_parser(i)\n",
    "#         print(i)\n",
    "#         print(soup)\n",
    "        if type(soup) is str:\n",
    "            print(soup)\n",
    "            return \n",
    "        if '404' in soup('title')[0].get_text():\n",
    "            broken.append(i)\n",
    "        else:\n",
    "            try:\n",
    "                title=soup.title.string\n",
    "            except:\n",
    "                title=''\n",
    "            try:\n",
    "                desc=soup.find('meta', attrs={'name': 'description'}).attrs['content']\n",
    "            except:\n",
    "                desc=''\n",
    "            page_data=subtitle1(soup)\n",
    "            page_source=\"None\"\n",
    "            if i[i.find('/content/'):].split('/')[0] is not None:\n",
    "                page_source=i[i.find('/content/'):].split('/')[2]\n",
    "                \n",
    "#             print(desc)\n",
    "        \n",
    "#             print(soup)\n",
    "            leaf_node=0\n",
    "#             if i in leaves:\n",
    "#                 leaf_node=1\n",
    "#             try:\n",
    "#                 publish_date=soup.find('meta', attrs={'name': 'publishDate'}).attrs['content']\n",
    "#             except:\n",
    "#                 publish_date=''\n",
    "#             print(publish_date)\n",
    "            if(date_modified):\n",
    "                try:\n",
    "                    difference=(datetime.today()-datetime.strptime(date_modified, \"%Y-%m-%dT%H:%M:%S.%fZ\" )).days\n",
    "                except:\n",
    "                    difference=(datetime.today()-datetime.strptime(date_modified, \"%Y-%m-%dT%H:%M:%SZ\" )).days\n",
    "            else:\n",
    "                try:\n",
    "                    difference=(datetime.today()-datetime.strptime(modified_date, \"%Y-%m-%dT%H:%M:%S.%fZ\" )).days\n",
    "                except:\n",
    "                    difference=(datetime.today()-datetime.strptime(modified_date, \"%Y-%m-%dT%H:%M:%SZ\" )).days\n",
    "            \n",
    "#             if difference<2:\n",
    "#                 env=\"dev\"\n",
    "#                 db_name = 'cognitive-service-'+env\n",
    "#                 conn = engine.connect()\n",
    "#                 query=\"select * from  where content_url=\\'\"+i+\"\\'\"\n",
    "#                 result=conn.execute(query)\n",
    "#                 re=result.fetchall()\n",
    "# #                 print(re)\n",
    "#                 if len(re)!=0:\n",
    "#                     temp=re[0][2]+re[0][3].replace(\",\",\" \")\n",
    "#                     comp=page_data['text']+page_data['subtitles'].replace(\",\",\" \")\n",
    "#                     similarity=(cosine_sim(temp,comp))\n",
    "                    \n",
    "#                     print(similarity)\n",
    "#                     if(similarity<0.9):\n",
    "#                         content_updated=1\n",
    "#                         row_updated={'content_url':i,'page_title':title,'page_text':page_data['text'],'page_sub_titles':page_data['subtitles'],'page_desc':desc,'leaf_node':1,'page_source':page_source,'parse_date':today,\"content_updated_recently\":content_updated}\n",
    "#                         data_updated.append(row_updated)\n",
    "#                     else:\n",
    "#                         content_updated=0\n",
    "#                 else:\n",
    "#                     content_updated=0\n",
    "#                 return re\n",
    "#             else:\n",
    "#                 content_updated=0\n",
    "            \n",
    "                \n",
    "            \n",
    "            row={'content_url':i,'page_title':title,'page_text':page_data['text'],'page_sub_titles':page_data['subtitles'],'page_desc':desc,'leaf_node':1,'page_source':page_source,'parse_date':today,\"content_updated_recently\":content_updated}\n",
    "#             print(row)\n",
    "#            pd.DataFrame(row).to_sql(con = conn, name = 'LCS_content_parsing', if_exists = 'append')\n",
    "#             print(row)\n",
    "            data.append(row)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "#         print(i)\n",
    "        failed_urls.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not a valid htmlnot a valid html\n",
      "not a valid html\n",
      "\n",
      "not a valid html\n",
      "HTTPSConnectionPool(host='electronicjournal.prod.walmart.com', port=443): Max retries exceeded with url: / (Caused by SSLError(SSLError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)'),))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=3, read=3, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:777)'),)': /v1/secret?token=d30753a60c0e53c922de2f38ea64528c9a9deef76897101aaeec7d4ab03ae4fe&deploymentType=project&deploymentId=5444&projectId=5444&KeyName=lcs_db_uid\n"
     ]
    }
   ],
   "source": [
    "#implement 3500 threads to run content extraction for 3000 urls\n",
    "pool=ThreadPool(2000)\n",
    "pool.map(myfn,df.head(10000).to_numpy())\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #extracted data to dataframe for easy access\n",
    "data=pd.DataFrame(data)\n",
    "data_updated=pd.DataFrame(data_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13796, 2)\n",
      "(95, 9)\n",
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(data.shape)\n",
    "print(data_updated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data=pd.read_excel('content_extraction_updated.xlsx')\n",
    "df_data=pd.concat([df_data,data_updated])\n",
    "df_data\n",
    "days=datetime.today() - relativedelta( days = +10 )\n",
    "# days\n",
    "df_data_ten_days = df_data[df_data['parse_date'] >= days]\n",
    "df_data_ten_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_updated.to_excel('content_extraction_updated.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete data from database\n",
    "def db_execute(query):\n",
    "    cxn = engine.connect()\n",
    "    trans = cxn.begin()\n",
    "    cxn.execute(query)\n",
    "    trans.commit()\n",
    "    cxn.close()\n",
    "db_execute('delete from [dbo].[LCS_content_parsing] where 1=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create connection to dev mysql server\n",
    "env=\"dev\"\n",
    "db_name = 'cognitive-service-'+env\n",
    "\n",
    "engine = create_engine()\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch insertion of data into mysql server\n",
    "data.to_sql(con = conn, name = 'table_name', index = False, if_exists = 'append')\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
